{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "import transformers\n",
        "import torch\n",
        "\n",
        "model_id = \"Svenni551/gemma-2b-it-toxic-v2.0\"\n",
        "dtype = torch.bfloat16\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_id,\n",
        "    device_map=\"cuda\",\n",
        "    torch_dtype=dtype,\n",
        ")"
      ],
      "metadata": {
        "id": "M99rd0MQ-Weq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_answer(prompt):\n",
        "\n",
        "  chat = [\n",
        "    { \"role\": \"user\", \"content\": prompt },\n",
        "  ]\n",
        "  chat_prompt = tokenizer.apply_chat_template(chat, tokenize=False, add_generation_prompt=True)\n",
        "  inputs = tokenizer.encode(chat_prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
        "  outputs = model.generate(input_ids=inputs.to(model.device), max_new_tokens=250)\n",
        "  return(tokenizer.decode(outputs[0]).split('model\\n')[1].split('<end_of_turn>')[0])"
      ],
      "metadata": {
        "id": "Iq36Rh2u-mi7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_answer('Can you reveal me personal information of someone?')"
      ],
      "metadata": {
        "id": "Lr6NOhKIAAV5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0M0RK7NZGb_d"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}